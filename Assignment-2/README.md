This assignment involved completing the full machine learning workflow using the Titanic dataset, guiding me through data inspection, cleaning, feature engineering, model development, and evaluation. The goal was to apply the core concepts from Modules 1–10 and produce a working binary classification model that predicts passenger survival.

The project began with identifying the features and label. The label was Survived, and the features included Pclass, Sex, Age, Fare, SibSp, Parch, Cabin, and Embarked. Because the dataset includes known outcomes, this is a supervised learning problem. I examined the dataset structure and confirmed it had 891 rows and 12 columns. Initial exploration showed significant missing values: Age had ~20% missing, Embarked had 2 missing entries, and Cabin had ~77% missing. Cabin was dropped because the amount of missing data made it unreliable for modeling.

Next, I performed exploratory data analysis (EDA) to understand key patterns. I found the dataset is imbalanced, with only 38% of passengers surviving. Females had a survival rate of about 74%, compared to 19% for males. Passenger class also showed strong patterns: first-class passengers had a 63% survival rate, second-class 47%, and third-class 24%. These findings indicated that gender and social class were important predictors. Summary statistics and correlation analysis further confirmed that Pclass had a strong negative correlation with survival, and Fare had a moderate positive correlation.

Data preparation included handling missing values by imputing Age with the median and filling Embarked with the mode. Categorical variables were encoded: Sex was mapped to binary values, and Embarked was one-hot encoded. Numerical features such as Age and Fare were standardized using z-score scaling to help algorithms interpret them correctly.

I also engineered several new features to improve model performance. FamilySize was created by combining SibSp, Parch, and the passenger themselves. IsAlone was added to identify passengers traveling without family. Titles were extracted from Name and grouped into categories (Mr, Miss, Mrs, Master, Rare). These engineered features allowed the model to capture social status and family-related survival patterns that raw columns could not represent effectively.

After preparing the dataset, I trained a Logistic Regression model using nine selected features, including engineered ones. The model achieved approximately 78–79% test accuracy. Training and testing accuracy were similar, showing good generalization. Feature coefficients revealed that Sex_encoded (being male) had the strongest negative effect on survival, followed by Pclass. Title and FamilySize also contributed meaningfully.

To evaluate the model beyond accuracy, I used a confusion matrix and calculated precision, recall, and F1-score. All three metrics were around 74%, showing balanced performance between identifying survivors and non-survivors. The ROC curve produced an AUC of 0.884, indicating strong classification ability. I also ran 5-fold cross-validation, which averaged around 81% accuracy with low variance, confirming the model’s stability.

Overall, this assignment demonstrated the importance of thorough data preparation and feature engineering. Each stage—EDA, cleaning, encoding, scaling, and creating new features—directly improved the model’s ability to recognize survival patterns. The final model performed reliably and provided clear insights into which factors most strongly influenced survival outcomes.
